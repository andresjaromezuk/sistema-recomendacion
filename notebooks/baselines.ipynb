{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import Model, Sequential\n",
    "from keras.layers import Embedding, Input, Flatten, Dot, Add\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1997-12-04 15:55:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1998-04-04 19:22:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-11-07 07:18:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1997-11-27 05:02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-02-02 05:33:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  user_id  movie_id  rating                 Date\n",
       "0   0        1         1       3  1997-12-04 15:55:49\n",
       "1   1        2         2       3  1998-04-04 19:22:22\n",
       "2   2        3         3       1  1997-11-07 07:18:36\n",
       "3   3        4         4       2  1997-11-27 05:02:03\n",
       "4   4        5         5       1  1998-02-02 05:33:16"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv('../data/scores.csv')\n",
    "df_users = pd.read_csv('../data/usuarios.csv')\n",
    "df_movies = pd.read_csv('../data/peliculas.csv')\n",
    "\n",
    "df_movies.loc[df_movies['IMDB URL'].isna(), 'IMDB URL'] = ''\n",
    "\n",
    "u_unique = ratings.user_id.unique()\n",
    "user2Idx = {o:i+1 for i,o in enumerate(u_unique)}\n",
    "\n",
    "m_unique = ratings.movie_id.unique()\n",
    "movie2Idx = {o:i+1 for i,o in enumerate(m_unique)}\n",
    "\n",
    "ratings.user_id = ratings.user_id.apply(lambda x: user2Idx[x])\n",
    "\n",
    "ratings.movie_id = ratings.movie_id.apply(lambda x: movie2Idx[x])\n",
    "\n",
    "ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ratings_train, ratings_val = train_test_split(ratings, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 1682 943 1655\n"
     ]
    }
   ],
   "source": [
    "n_users = int(ratings.user_id.nunique())\n",
    "n_movies = int(ratings.movie_id.nunique())\n",
    "n_users_train = int(ratings_train.user_id.nunique())\n",
    "n_movies_train = int(ratings_train.movie_id.nunique())\n",
    "print(n_users, n_movies, n_users_train, n_movies_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/03 19:26:57 INFO mlflow.tracking.fluent: Experiment with name 'Baselines' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/826553565838014976', creation_time=1730672817055, experiment_id='826553565838014976', last_update_time=1730672817055, lifecycle_stage='active', name='Baselines', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seteo del experimento\n",
    "experiment_name = \"Baselines\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.start_run(run_name=\"Early Stopping + latent factor 5 + lr 0.0014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_latent_factors = 5\n",
    "mlflow.log_param(\"n_latent_factors\", n_latent_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2_reg = l2(0.00025)\n",
    "movie_embedding_regularizer = 0.001\n",
    "l2_reg = l2(0.00)\n",
    "mlflow.log_param(\"movie_embedding_regularizer_l2\", movie_embedding_regularizer)\n",
    "mlflow.log_param(\"user_embedding_regularizer_l2\", 0.00)\n",
    "movie_input = Input(shape=[1], name='Item')\n",
    "movie_embedding = Embedding(n_movies + 1, \n",
    "                            n_latent_factors, \n",
    "                            embeddings_regularizer = l2(movie_embedding_regularizer),\n",
    "                            name='Movie-Embedding')(movie_input)\n",
    "movie_vec = Flatten(name='FlattenMovies')(movie_embedding)\n",
    "\n",
    "m_biases = Flatten(name='movie_biases_flt')(Embedding(n_movies + 1, 1, name=\"movie_biases\", embeddings_regularizer = l2_reg)(movie_input))\n",
    "\n",
    "user_input = Input(shape=[1],name='User')\n",
    "user_vec = Flatten(name='FlattenUsers')(Embedding(n_users + 1, n_latent_factors,embeddings_regularizer = l2_reg,name='User-Embedding')(user_input))\n",
    "u_biases = Flatten(name='user_biases_flt')(Embedding(n_users + 1, 1, name=\"user_biases\", embeddings_regularizer = l2_reg)(user_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Item (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " User (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Movie-Embedding (Embedding)    (None, 1, 5)         8415        ['Item[0][0]']                   \n",
      "                                                                                                  \n",
      " User-Embedding (Embedding)     (None, 1, 5)         4720        ['User[0][0]']                   \n",
      "                                                                                                  \n",
      " FlattenMovies (Flatten)        (None, 5)            0           ['Movie-Embedding[0][0]']        \n",
      "                                                                                                  \n",
      " FlattenUsers (Flatten)         (None, 5)            0           ['User-Embedding[0][0]']         \n",
      "                                                                                                  \n",
      " user_biases (Embedding)        (None, 1, 1)         944         ['User[0][0]']                   \n",
      "                                                                                                  \n",
      " movie_biases (Embedding)       (None, 1, 1)         1683        ['Item[0][0]']                   \n",
      "                                                                                                  \n",
      " DotProduct (Dot)               (None, 1)            0           ['FlattenMovies[0][0]',          \n",
      "                                                                  'FlattenUsers[0][0]']           \n",
      "                                                                                                  \n",
      " user_biases_flt (Flatten)      (None, 1)            0           ['user_biases[0][0]']            \n",
      "                                                                                                  \n",
      " movie_biases_flt (Flatten)     (None, 1)            0           ['movie_biases[0][0]']           \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 1)            0           ['DotProduct[0][0]',             \n",
      "                                                                  'user_biases_flt[0][0]',        \n",
      "                                                                  'movie_biases_flt[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,762\n",
      "Trainable params: 15,762\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prod = Dot(axes=1, name='DotProduct')([movie_vec, user_vec])\n",
    "out = Add()([prod, u_biases, m_biases])\n",
    "model = Model([user_input, movie_input], out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0014"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.0014\n",
    "model.compile(Adam(learning_rate=lr), 'mean_squared_error', metrics=[root_mean_squared_error])\n",
    "mlflow.log_param(\"lr\", lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='weights1.hdf5', verbose=1, save_best_only=True, monitor='val_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "patience = 5\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "mlflow.log_param(\"early_stopping_patience\", patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "232/250 [==========================>...] - ETA: 0s - loss: 12.4487 - root_mean_squared_error: 3.5251\n",
      "Epoch 1: val_root_mean_squared_error improved from inf to 3.25344, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 12.3260 - root_mean_squared_error: 3.5068 - val_loss: 10.6374 - val_root_mean_squared_error: 3.2534\n",
      "Epoch 2/100\n",
      "212/250 [========================>.....] - ETA: 0s - loss: 8.3827 - root_mean_squared_error: 2.8465\n",
      "Epoch 2: val_root_mean_squared_error improved from 3.25344 to 2.27224, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 982us/step - loss: 8.0026 - root_mean_squared_error: 2.7684 - val_loss: 5.6323 - val_root_mean_squared_error: 2.2722\n",
      "Epoch 3/100\n",
      "213/250 [========================>.....] - ETA: 0s - loss: 4.3690 - root_mean_squared_error: 1.9205\n",
      "Epoch 3: val_root_mean_squared_error improved from 2.27224 to 1.59860, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 970us/step - loss: 4.2259 - root_mean_squared_error: 1.8744 - val_loss: 3.3806 - val_root_mean_squared_error: 1.5986\n",
      "Epoch 4/100\n",
      "208/250 [=======================>......] - ETA: 0s - loss: 3.0012 - root_mean_squared_error: 1.4519\n",
      "Epoch 4: val_root_mean_squared_error improved from 1.59860 to 1.34544, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 977us/step - loss: 2.9565 - root_mean_squared_error: 1.4341 - val_loss: 2.7325 - val_root_mean_squared_error: 1.3454\n",
      "Epoch 5/100\n",
      "191/250 [=====================>........] - ETA: 0s - loss: 2.5444 - root_mean_squared_error: 1.2736\n",
      "Epoch 5: val_root_mean_squared_error improved from 1.34544 to 1.23259, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 2.5141 - root_mean_squared_error: 1.2634 - val_loss: 2.4125 - val_root_mean_squared_error: 1.2326\n",
      "Epoch 6/100\n",
      "196/250 [======================>.......] - ETA: 0s - loss: 2.2663 - root_mean_squared_error: 1.1808\n",
      "Epoch 6: val_root_mean_squared_error improved from 1.23259 to 1.16727, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 2.2521 - root_mean_squared_error: 1.1779 - val_loss: 2.1894 - val_root_mean_squared_error: 1.1673\n",
      "Epoch 7/100\n",
      "243/250 [============================>.] - ETA: 0s - loss: 2.0609 - root_mean_squared_error: 1.1261\n",
      "Epoch 7: val_root_mean_squared_error improved from 1.16727 to 1.12464, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 2.0591 - root_mean_squared_error: 1.1258 - val_loss: 2.0176 - val_root_mean_squared_error: 1.1246\n",
      "Epoch 8/100\n",
      "198/250 [======================>.......] - ETA: 0s - loss: 1.9154 - root_mean_squared_error: 1.0912\n",
      "Epoch 8: val_root_mean_squared_error improved from 1.12464 to 1.09385, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.9043 - root_mean_squared_error: 1.0896 - val_loss: 1.8755 - val_root_mean_squared_error: 1.0939\n",
      "Epoch 9/100\n",
      "185/250 [=====================>........] - ETA: 0s - loss: 1.7856 - root_mean_squared_error: 1.0638\n",
      "Epoch 9: val_root_mean_squared_error improved from 1.09385 to 1.07033, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.7766 - root_mean_squared_error: 1.0636 - val_loss: 1.7577 - val_root_mean_squared_error: 1.0703\n",
      "Epoch 10/100\n",
      "237/250 [===========================>..] - ETA: 0s - loss: 1.6722 - root_mean_squared_error: 1.0432\n",
      "Epoch 10: val_root_mean_squared_error improved from 1.07033 to 1.05276, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.6681 - root_mean_squared_error: 1.0420 - val_loss: 1.6567 - val_root_mean_squared_error: 1.0528\n",
      "Epoch 11/100\n",
      "236/250 [===========================>..] - ETA: 0s - loss: 1.5759 - root_mean_squared_error: 1.0259\n",
      "Epoch 11: val_root_mean_squared_error improved from 1.05276 to 1.03714, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.5754 - root_mean_squared_error: 1.0265 - val_loss: 1.5695 - val_root_mean_squared_error: 1.0371\n",
      "Epoch 12/100\n",
      "189/250 [=====================>........] - ETA: 0s - loss: 1.4989 - root_mean_squared_error: 1.0110\n",
      "Epoch 12: val_root_mean_squared_error improved from 1.03714 to 1.02441, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.4955 - root_mean_squared_error: 1.0124 - val_loss: 1.4942 - val_root_mean_squared_error: 1.0244\n",
      "Epoch 13/100\n",
      "210/250 [========================>.....] - ETA: 0s - loss: 1.4281 - root_mean_squared_error: 1.0003\n",
      "Epoch 13: val_root_mean_squared_error improved from 1.02441 to 1.01356, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 971us/step - loss: 1.4257 - root_mean_squared_error: 1.0009 - val_loss: 1.4286 - val_root_mean_squared_error: 1.0136\n",
      "Epoch 14/100\n",
      "206/250 [=======================>......] - ETA: 0s - loss: 1.3650 - root_mean_squared_error: 0.9896\n",
      "Epoch 14: val_root_mean_squared_error improved from 1.01356 to 1.00474, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 991us/step - loss: 1.3648 - root_mean_squared_error: 0.9912 - val_loss: 1.3714 - val_root_mean_squared_error: 1.0047\n",
      "Epoch 15/100\n",
      "199/250 [======================>.......] - ETA: 0s - loss: 1.3129 - root_mean_squared_error: 0.9820\n",
      "Epoch 15: val_root_mean_squared_error improved from 1.00474 to 0.99648, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.3111 - root_mean_squared_error: 0.9829 - val_loss: 1.3210 - val_root_mean_squared_error: 0.9965\n",
      "Epoch 16/100\n",
      "211/250 [========================>.....] - ETA: 0s - loss: 1.2627 - root_mean_squared_error: 0.9735\n",
      "Epoch 16: val_root_mean_squared_error improved from 0.99648 to 0.98953, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 992us/step - loss: 1.2644 - root_mean_squared_error: 0.9756 - val_loss: 1.2761 - val_root_mean_squared_error: 0.9895\n",
      "Epoch 17/100\n",
      "199/250 [======================>.......] - ETA: 0s - loss: 1.2271 - root_mean_squared_error: 0.9700\n",
      "Epoch 17: val_root_mean_squared_error improved from 0.98953 to 0.98344, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.2227 - root_mean_squared_error: 0.9691 - val_loss: 1.2369 - val_root_mean_squared_error: 0.9834\n",
      "Epoch 18/100\n",
      "198/250 [======================>.......] - ETA: 0s - loss: 1.1861 - root_mean_squared_error: 0.9623\n",
      "Epoch 18: val_root_mean_squared_error improved from 0.98344 to 0.97836, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.1856 - root_mean_squared_error: 0.9634 - val_loss: 1.2026 - val_root_mean_squared_error: 0.9784\n",
      "Epoch 19/100\n",
      "192/250 [======================>.......] - ETA: 0s - loss: 1.1557 - root_mean_squared_error: 0.9582\n",
      "Epoch 19: val_root_mean_squared_error improved from 0.97836 to 0.97411, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.1530 - root_mean_squared_error: 0.9582 - val_loss: 1.1715 - val_root_mean_squared_error: 0.9741\n",
      "Epoch 20/100\n",
      "206/250 [=======================>......] - ETA: 0s - loss: 1.1240 - root_mean_squared_error: 0.9533\n",
      "Epoch 20: val_root_mean_squared_error improved from 0.97411 to 0.96997, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 984us/step - loss: 1.1239 - root_mean_squared_error: 0.9541 - val_loss: 1.1437 - val_root_mean_squared_error: 0.9700\n",
      "Epoch 21/100\n",
      "205/250 [=======================>......] - ETA: 0s - loss: 1.0965 - root_mean_squared_error: 0.9486\n",
      "Epoch 21: val_root_mean_squared_error improved from 0.96997 to 0.96606, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 986us/step - loss: 1.0977 - root_mean_squared_error: 0.9501 - val_loss: 1.1185 - val_root_mean_squared_error: 0.9661\n",
      "Epoch 22/100\n",
      "206/250 [=======================>......] - ETA: 0s - loss: 1.0724 - root_mean_squared_error: 0.9450\n",
      "Epoch 22: val_root_mean_squared_error improved from 0.96606 to 0.96296, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.0746 - root_mean_squared_error: 0.9468 - val_loss: 1.0970 - val_root_mean_squared_error: 0.9630\n",
      "Epoch 23/100\n",
      "205/250 [=======================>......] - ETA: 0s - loss: 1.0560 - root_mean_squared_error: 0.9443\n",
      "Epoch 23: val_root_mean_squared_error improved from 0.96296 to 0.95962, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 997us/step - loss: 1.0537 - root_mean_squared_error: 0.9438 - val_loss: 1.0771 - val_root_mean_squared_error: 0.9596\n",
      "Epoch 24/100\n",
      "211/250 [========================>.....] - ETA: 0s - loss: 1.0331 - root_mean_squared_error: 0.9390\n",
      "Epoch 24: val_root_mean_squared_error improved from 0.95962 to 0.95779, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 984us/step - loss: 1.0350 - root_mean_squared_error: 0.9406 - val_loss: 1.0605 - val_root_mean_squared_error: 0.9578\n",
      "Epoch 25/100\n",
      "201/250 [=======================>......] - ETA: 0s - loss: 1.0195 - root_mean_squared_error: 0.9385\n",
      "Epoch 25: val_root_mean_squared_error improved from 0.95779 to 0.95494, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1000us/step - loss: 1.0183 - root_mean_squared_error: 0.9385 - val_loss: 1.0441 - val_root_mean_squared_error: 0.9549\n",
      "Epoch 26/100\n",
      "203/250 [=======================>......] - ETA: 0s - loss: 1.0006 - root_mean_squared_error: 0.9342\n",
      "Epoch 26: val_root_mean_squared_error improved from 0.95494 to 0.95341, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 992us/step - loss: 1.0030 - root_mean_squared_error: 0.9360 - val_loss: 1.0307 - val_root_mean_squared_error: 0.9534\n",
      "Epoch 27/100\n",
      "204/250 [=======================>......] - ETA: 0s - loss: 0.9902 - root_mean_squared_error: 0.9341\n",
      "Epoch 27: val_root_mean_squared_error improved from 0.95341 to 0.95097, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 995us/step - loss: 0.9892 - root_mean_squared_error: 0.9340 - val_loss: 1.0169 - val_root_mean_squared_error: 0.9510\n",
      "Epoch 28/100\n",
      "198/250 [======================>.......] - ETA: 0s - loss: 0.9784 - root_mean_squared_error: 0.9322\n",
      "Epoch 28: val_root_mean_squared_error improved from 0.95097 to 0.94962, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.9769 - root_mean_squared_error: 0.9319 - val_loss: 1.0057 - val_root_mean_squared_error: 0.9496\n",
      "Epoch 29/100\n",
      "205/250 [=======================>......] - ETA: 0s - loss: 0.9617 - root_mean_squared_error: 0.9280\n",
      "Epoch 29: val_root_mean_squared_error improved from 0.94962 to 0.94823, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 989us/step - loss: 0.9656 - root_mean_squared_error: 0.9304 - val_loss: 0.9959 - val_root_mean_squared_error: 0.9482\n",
      "Epoch 30/100\n",
      "203/250 [=======================>......] - ETA: 0s - loss: 0.9519 - root_mean_squared_error: 0.9264\n",
      "Epoch 30: val_root_mean_squared_error improved from 0.94823 to 0.94605, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.9556 - root_mean_squared_error: 0.9287 - val_loss: 0.9849 - val_root_mean_squared_error: 0.9461\n",
      "Epoch 31/100\n",
      "185/250 [=====================>........] - ETA: 0s - loss: 0.9452 - root_mean_squared_error: 0.9262\n",
      "Epoch 31: val_root_mean_squared_error improved from 0.94605 to 0.94516, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.9455 - root_mean_squared_error: 0.9268 - val_loss: 0.9775 - val_root_mean_squared_error: 0.9452\n",
      "Epoch 32/100\n",
      "205/250 [=======================>......] - ETA: 0s - loss: 0.9360 - root_mean_squared_error: 0.9244\n",
      "Epoch 32: val_root_mean_squared_error improved from 0.94516 to 0.94393, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.9373 - root_mean_squared_error: 0.9254 - val_loss: 0.9690 - val_root_mean_squared_error: 0.9439\n",
      "Epoch 33/100\n",
      "207/250 [=======================>......] - ETA: 0s - loss: 0.9261 - root_mean_squared_error: 0.9223\n",
      "Epoch 33: val_root_mean_squared_error improved from 0.94393 to 0.94290, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 988us/step - loss: 0.9295 - root_mean_squared_error: 0.9243 - val_loss: 0.9618 - val_root_mean_squared_error: 0.9429\n",
      "Epoch 34/100\n",
      "201/250 [=======================>......] - ETA: 0s - loss: 0.9237 - root_mean_squared_error: 0.9234\n",
      "Epoch 34: val_root_mean_squared_error improved from 0.94290 to 0.94153, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.9222 - root_mean_squared_error: 0.9229 - val_loss: 0.9544 - val_root_mean_squared_error: 0.9415\n",
      "Epoch 35/100\n",
      "199/250 [======================>.......] - ETA: 0s - loss: 0.9126 - root_mean_squared_error: 0.9200\n",
      "Epoch 35: val_root_mean_squared_error improved from 0.94153 to 0.94090, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.9158 - root_mean_squared_error: 0.9220 - val_loss: 0.9491 - val_root_mean_squared_error: 0.9409\n",
      "Epoch 36/100\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.9096 - root_mean_squared_error: 0.9207\n",
      "Epoch 36: val_root_mean_squared_error improved from 0.94090 to 0.94028, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.9094 - root_mean_squared_error: 0.9206 - val_loss: 0.9442 - val_root_mean_squared_error: 0.9403\n",
      "Epoch 37/100\n",
      "197/250 [======================>.......] - ETA: 0s - loss: 0.8993 - root_mean_squared_error: 0.9170\n",
      "Epoch 37: val_root_mean_squared_error improved from 0.94028 to 0.93907, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.9037 - root_mean_squared_error: 0.9196 - val_loss: 0.9385 - val_root_mean_squared_error: 0.9391\n",
      "Epoch 38/100\n",
      "206/250 [=======================>......] - ETA: 0s - loss: 0.9000 - root_mean_squared_error: 0.9190\n",
      "Epoch 38: val_root_mean_squared_error improved from 0.93907 to 0.93856, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 983us/step - loss: 0.8983 - root_mean_squared_error: 0.9183 - val_loss: 0.9341 - val_root_mean_squared_error: 0.9386\n",
      "Epoch 39/100\n",
      "243/250 [============================>.] - ETA: 0s - loss: 0.8919 - root_mean_squared_error: 0.9167\n",
      "Epoch 39: val_root_mean_squared_error improved from 0.93856 to 0.93783, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8927 - root_mean_squared_error: 0.9172 - val_loss: 0.9300 - val_root_mean_squared_error: 0.9378\n",
      "Epoch 40/100\n",
      "189/250 [=====================>........] - ETA: 0s - loss: 0.8861 - root_mean_squared_error: 0.9148\n",
      "Epoch 40: val_root_mean_squared_error improved from 0.93783 to 0.93708, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8884 - root_mean_squared_error: 0.9162 - val_loss: 0.9259 - val_root_mean_squared_error: 0.9371\n",
      "Epoch 41/100\n",
      "202/250 [=======================>......] - ETA: 0s - loss: 0.8813 - root_mean_squared_error: 0.9135\n",
      "Epoch 41: val_root_mean_squared_error improved from 0.93708 to 0.93630, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 998us/step - loss: 0.8835 - root_mean_squared_error: 0.9149 - val_loss: 0.9220 - val_root_mean_squared_error: 0.9363\n",
      "Epoch 42/100\n",
      "206/250 [=======================>......] - ETA: 0s - loss: 0.8784 - root_mean_squared_error: 0.9131\n",
      "Epoch 42: val_root_mean_squared_error improved from 0.93630 to 0.93586, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 988us/step - loss: 0.8794 - root_mean_squared_error: 0.9139 - val_loss: 0.9190 - val_root_mean_squared_error: 0.9359\n",
      "Epoch 43/100\n",
      "207/250 [=======================>......] - ETA: 0s - loss: 0.8729 - root_mean_squared_error: 0.9116\n",
      "Epoch 43: val_root_mean_squared_error improved from 0.93586 to 0.93424, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 988us/step - loss: 0.8752 - root_mean_squared_error: 0.9129 - val_loss: 0.9144 - val_root_mean_squared_error: 0.9342\n",
      "Epoch 44/100\n",
      "204/250 [=======================>......] - ETA: 0s - loss: 0.8708 - root_mean_squared_error: 0.9112\n",
      "Epoch 44: val_root_mean_squared_error improved from 0.93424 to 0.93345, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 989us/step - loss: 0.8709 - root_mean_squared_error: 0.9114 - val_loss: 0.9111 - val_root_mean_squared_error: 0.9334\n",
      "Epoch 45/100\n",
      "202/250 [=======================>......] - ETA: 0s - loss: 0.8671 - root_mean_squared_error: 0.9100\n",
      "Epoch 45: val_root_mean_squared_error improved from 0.93345 to 0.93318, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 994us/step - loss: 0.8672 - root_mean_squared_error: 0.9103 - val_loss: 0.9089 - val_root_mean_squared_error: 0.9332\n",
      "Epoch 46/100\n",
      "244/250 [============================>.] - ETA: 0s - loss: 0.8612 - root_mean_squared_error: 0.9077\n",
      "Epoch 46: val_root_mean_squared_error improved from 0.93318 to 0.93154, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8627 - root_mean_squared_error: 0.9085 - val_loss: 0.9046 - val_root_mean_squared_error: 0.9315\n",
      "Epoch 47/100\n",
      "201/250 [=======================>......] - ETA: 0s - loss: 0.8583 - root_mean_squared_error: 0.9067\n",
      "Epoch 47: val_root_mean_squared_error improved from 0.93154 to 0.93105, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.8588 - root_mean_squared_error: 0.9070 - val_loss: 0.9026 - val_root_mean_squared_error: 0.9310\n",
      "Epoch 48/100\n",
      "195/250 [======================>.......] - ETA: 0s - loss: 0.8542 - root_mean_squared_error: 0.9051\n",
      "Epoch 48: val_root_mean_squared_error improved from 0.93105 to 0.92942, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8542 - root_mean_squared_error: 0.9050 - val_loss: 0.8991 - val_root_mean_squared_error: 0.9294\n",
      "Epoch 49/100\n",
      "195/250 [======================>.......] - ETA: 0s - loss: 0.8500 - root_mean_squared_error: 0.9032\n",
      "Epoch 49: val_root_mean_squared_error improved from 0.92942 to 0.92864, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8500 - root_mean_squared_error: 0.9033 - val_loss: 0.8968 - val_root_mean_squared_error: 0.9286\n",
      "Epoch 50/100\n",
      "189/250 [=====================>........] - ETA: 0s - loss: 0.8460 - root_mean_squared_error: 0.9013\n",
      "Epoch 50: val_root_mean_squared_error improved from 0.92864 to 0.92682, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8456 - root_mean_squared_error: 0.9010 - val_loss: 0.8930 - val_root_mean_squared_error: 0.9268\n",
      "Epoch 51/100\n",
      "206/250 [=======================>......] - ETA: 0s - loss: 0.8399 - root_mean_squared_error: 0.8980\n",
      "Epoch 51: val_root_mean_squared_error improved from 0.92682 to 0.92675, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 988us/step - loss: 0.8409 - root_mean_squared_error: 0.8987 - val_loss: 0.8922 - val_root_mean_squared_error: 0.9268\n",
      "Epoch 52/100\n",
      "204/250 [=======================>......] - ETA: 0s - loss: 0.8347 - root_mean_squared_error: 0.8953\n",
      "Epoch 52: val_root_mean_squared_error improved from 0.92675 to 0.92434, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 990us/step - loss: 0.8365 - root_mean_squared_error: 0.8963 - val_loss: 0.8878 - val_root_mean_squared_error: 0.9243\n",
      "Epoch 53/100\n",
      "221/250 [=========================>....] - ETA: 0s - loss: 0.8324 - root_mean_squared_error: 0.8941\n",
      "Epoch 53: val_root_mean_squared_error improved from 0.92434 to 0.92236, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8316 - root_mean_squared_error: 0.8937 - val_loss: 0.8843 - val_root_mean_squared_error: 0.9224\n",
      "Epoch 54/100\n",
      "187/250 [=====================>........] - ETA: 0s - loss: 0.8233 - root_mean_squared_error: 0.8890\n",
      "Epoch 54: val_root_mean_squared_error improved from 0.92236 to 0.92148, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8271 - root_mean_squared_error: 0.8910 - val_loss: 0.8828 - val_root_mean_squared_error: 0.9215\n",
      "Epoch 55/100\n",
      "198/250 [======================>.......] - ETA: 0s - loss: 0.8198 - root_mean_squared_error: 0.8870\n",
      "Epoch 55: val_root_mean_squared_error improved from 0.92148 to 0.92021, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8225 - root_mean_squared_error: 0.8884 - val_loss: 0.8807 - val_root_mean_squared_error: 0.9202\n",
      "Epoch 56/100\n",
      "184/250 [=====================>........] - ETA: 0s - loss: 0.8170 - root_mean_squared_error: 0.8853\n",
      "Epoch 56: val_root_mean_squared_error improved from 0.92021 to 0.91888, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8185 - root_mean_squared_error: 0.8860 - val_loss: 0.8786 - val_root_mean_squared_error: 0.9189\n",
      "Epoch 57/100\n",
      "200/250 [=======================>......] - ETA: 0s - loss: 0.8161 - root_mean_squared_error: 0.8848\n",
      "Epoch 57: val_root_mean_squared_error improved from 0.91888 to 0.91724, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8144 - root_mean_squared_error: 0.8837 - val_loss: 0.8759 - val_root_mean_squared_error: 0.9172\n",
      "Epoch 58/100\n",
      "209/250 [========================>.....] - ETA: 0s - loss: 0.8063 - root_mean_squared_error: 0.8787\n",
      "Epoch 58: val_root_mean_squared_error improved from 0.91724 to 0.91640, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 980us/step - loss: 0.8102 - root_mean_squared_error: 0.8809 - val_loss: 0.8744 - val_root_mean_squared_error: 0.9164\n",
      "Epoch 59/100\n",
      "190/250 [=====================>........] - ETA: 0s - loss: 0.8073 - root_mean_squared_error: 0.8793\n",
      "Epoch 59: val_root_mean_squared_error improved from 0.91640 to 0.91529, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8068 - root_mean_squared_error: 0.8789 - val_loss: 0.8728 - val_root_mean_squared_error: 0.9153\n",
      "Epoch 60/100\n",
      "203/250 [=======================>......] - ETA: 0s - loss: 0.8017 - root_mean_squared_error: 0.8759\n",
      "Epoch 60: val_root_mean_squared_error improved from 0.91529 to 0.91511, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 997us/step - loss: 0.8032 - root_mean_squared_error: 0.8768 - val_loss: 0.8725 - val_root_mean_squared_error: 0.9151\n",
      "Epoch 61/100\n",
      "202/250 [=======================>......] - ETA: 0s - loss: 0.7976 - root_mean_squared_error: 0.8736\n",
      "Epoch 61: val_root_mean_squared_error improved from 0.91511 to 0.91496, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.7999 - root_mean_squared_error: 0.8748 - val_loss: 0.8723 - val_root_mean_squared_error: 0.9150\n",
      "Epoch 62/100\n",
      "203/250 [=======================>......] - ETA: 0s - loss: 0.7938 - root_mean_squared_error: 0.8711\n",
      "Epoch 62: val_root_mean_squared_error improved from 0.91496 to 0.91405, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 989us/step - loss: 0.7963 - root_mean_squared_error: 0.8726 - val_loss: 0.8708 - val_root_mean_squared_error: 0.9141\n",
      "Epoch 63/100\n",
      "188/250 [=====================>........] - ETA: 0s - loss: 0.7905 - root_mean_squared_error: 0.8693\n",
      "Epoch 63: val_root_mean_squared_error improved from 0.91405 to 0.91345, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7937 - root_mean_squared_error: 0.8711 - val_loss: 0.8699 - val_root_mean_squared_error: 0.9134\n",
      "Epoch 64/100\n",
      "194/250 [======================>.......] - ETA: 0s - loss: 0.7858 - root_mean_squared_error: 0.8663\n",
      "Epoch 64: val_root_mean_squared_error improved from 0.91345 to 0.91291, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7906 - root_mean_squared_error: 0.8692 - val_loss: 0.8686 - val_root_mean_squared_error: 0.9129\n",
      "Epoch 65/100\n",
      "207/250 [=======================>......] - ETA: 0s - loss: 0.7852 - root_mean_squared_error: 0.8661\n",
      "Epoch 65: val_root_mean_squared_error did not improve from 0.91291\n",
      "250/250 [==============================] - 0s 927us/step - loss: 0.7878 - root_mean_squared_error: 0.8677 - val_loss: 0.8696 - val_root_mean_squared_error: 0.9133\n",
      "Epoch 66/100\n",
      "217/250 [=========================>....] - ETA: 0s - loss: 0.7852 - root_mean_squared_error: 0.8659\n",
      "Epoch 66: val_root_mean_squared_error improved from 0.91291 to 0.91258, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 958us/step - loss: 0.7850 - root_mean_squared_error: 0.8658 - val_loss: 0.8688 - val_root_mean_squared_error: 0.9126\n",
      "Epoch 67/100\n",
      "202/250 [=======================>......] - ETA: 0s - loss: 0.7818 - root_mean_squared_error: 0.8640\n",
      "Epoch 67: val_root_mean_squared_error improved from 0.91258 to 0.91234, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7820 - root_mean_squared_error: 0.8642 - val_loss: 0.8683 - val_root_mean_squared_error: 0.9123\n",
      "Epoch 68/100\n",
      "239/250 [===========================>..] - ETA: 0s - loss: 0.7785 - root_mean_squared_error: 0.8621\n",
      "Epoch 68: val_root_mean_squared_error improved from 0.91234 to 0.91203, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7794 - root_mean_squared_error: 0.8626 - val_loss: 0.8680 - val_root_mean_squared_error: 0.9120\n",
      "Epoch 69/100\n",
      "198/250 [======================>.......] - ETA: 0s - loss: 0.7755 - root_mean_squared_error: 0.8604\n",
      "Epoch 69: val_root_mean_squared_error improved from 0.91203 to 0.91134, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7765 - root_mean_squared_error: 0.8610 - val_loss: 0.8669 - val_root_mean_squared_error: 0.9113\n",
      "Epoch 70/100\n",
      "202/250 [=======================>......] - ETA: 0s - loss: 0.7723 - root_mean_squared_error: 0.8582\n",
      "Epoch 70: val_root_mean_squared_error improved from 0.91134 to 0.91098, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7738 - root_mean_squared_error: 0.8591 - val_loss: 0.8664 - val_root_mean_squared_error: 0.9110\n",
      "Epoch 71/100\n",
      "192/250 [======================>.......] - ETA: 0s - loss: 0.7693 - root_mean_squared_error: 0.8565\n",
      "Epoch 71: val_root_mean_squared_error improved from 0.91098 to 0.91069, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7713 - root_mean_squared_error: 0.8576 - val_loss: 0.8662 - val_root_mean_squared_error: 0.9107\n",
      "Epoch 72/100\n",
      "187/250 [=====================>........] - ETA: 0s - loss: 0.7630 - root_mean_squared_error: 0.8526\n",
      "Epoch 72: val_root_mean_squared_error improved from 0.91069 to 0.91059, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7685 - root_mean_squared_error: 0.8557 - val_loss: 0.8662 - val_root_mean_squared_error: 0.9106\n",
      "Epoch 73/100\n",
      "205/250 [=======================>......] - ETA: 0s - loss: 0.7656 - root_mean_squared_error: 0.8538\n",
      "Epoch 73: val_root_mean_squared_error improved from 0.91059 to 0.90997, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 995us/step - loss: 0.7657 - root_mean_squared_error: 0.8538 - val_loss: 0.8656 - val_root_mean_squared_error: 0.9100\n",
      "Epoch 74/100\n",
      "180/250 [====================>.........] - ETA: 0s - loss: 0.7621 - root_mean_squared_error: 0.8516\n",
      "Epoch 74: val_root_mean_squared_error improved from 0.90997 to 0.90960, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7632 - root_mean_squared_error: 0.8523 - val_loss: 0.8651 - val_root_mean_squared_error: 0.9096\n",
      "Epoch 75/100\n",
      "203/250 [=======================>......] - ETA: 0s - loss: 0.7611 - root_mean_squared_error: 0.8508\n",
      "Epoch 75: val_root_mean_squared_error did not improve from 0.90960\n",
      "250/250 [==============================] - 0s 988us/step - loss: 0.7605 - root_mean_squared_error: 0.8505 - val_loss: 0.8661 - val_root_mean_squared_error: 0.9100\n",
      "Epoch 76/100\n",
      "216/250 [========================>.....] - ETA: 0s - loss: 0.7591 - root_mean_squared_error: 0.8496\n",
      "Epoch 76: val_root_mean_squared_error did not improve from 0.90960\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.7581 - root_mean_squared_error: 0.8489 - val_loss: 0.8667 - val_root_mean_squared_error: 0.9100\n",
      "Epoch 77/100\n",
      "219/250 [=========================>....] - ETA: 0s - loss: 0.7546 - root_mean_squared_error: 0.8467\n",
      "Epoch 77: val_root_mean_squared_error improved from 0.90960 to 0.90913, saving model to weights1.hdf5\n",
      "250/250 [==============================] - 0s 948us/step - loss: 0.7555 - root_mean_squared_error: 0.8472 - val_loss: 0.8655 - val_root_mean_squared_error: 0.9091\n",
      "Epoch 78/100\n",
      "214/250 [========================>.....] - ETA: 0s - loss: 0.7540 - root_mean_squared_error: 0.8459\n",
      "Epoch 78: val_root_mean_squared_error did not improve from 0.90913\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.7531 - root_mean_squared_error: 0.8454 - val_loss: 0.8663 - val_root_mean_squared_error: 0.9093\n",
      "Epoch 79/100\n",
      "206/250 [=======================>......] - ETA: 0s - loss: 0.7482 - root_mean_squared_error: 0.8424\n",
      "Epoch 79: val_root_mean_squared_error did not improve from 0.90913\n",
      "250/250 [==============================] - 0s 942us/step - loss: 0.7504 - root_mean_squared_error: 0.8437 - val_loss: 0.8674 - val_root_mean_squared_error: 0.9098\n"
     ]
    }
   ],
   "source": [
    "batch_size = 320\n",
    "epochs = 100\n",
    "mlflow.log_param(\"batch_size\", batch_size)\n",
    "mlflow.log_param(\"epochs\", epochs)\n",
    "\n",
    "history = model.fit([ratings_train.user_id, ratings_train.movie_id], \n",
    "                    ratings_train.rating, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=([ratings_val.user_id, ratings_val.movie_id], ratings_val.rating), \n",
    "                    epochs=epochs, \n",
    "                    callbacks = [checkpointer, early_stopping],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in history.history.items():\n",
    "  mlflow.log_metric(key, value[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 602us/step - loss: 0.8651 - root_mean_squared_error: 0.9024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8651050925254822, 0.9024324417114258]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([ratings_val.user_id, ratings_val.movie_id], ratings_val.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 597us/step - loss: 0.8655 - root_mean_squared_error: 0.9020\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('weights1.hdf5')\n",
    "mse, rmse = model.evaluate([ratings_val.user_id, ratings_val.movie_id], ratings_val.rating)\n",
    "mlflow.log_metric(\"val_mse\", mse) \n",
    "mlflow.log_metric(\"val_rmse\", rmse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Movie-Embedding', 'User-Embedding')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_embeddings_layer = model.layers[2]\n",
    "user_embeddings_layer = model.layers[3]\n",
    "\n",
    "movie_embeddings_layer.name, user_embeddings_layer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/03 20:04:03 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) User, Item with unsupported characters which will be renamed to user, item in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/7f/19f36bv57_72qpq3mfj1x6240000gn/T/tmpuj_5ulg9/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/7f/19f36bv57_72qpq3mfj1x6240000gn/T/tmpuj_5ulg9/model/data/model/assets\n",
      "2024/11/03 20:04:08 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /var/folders/7f/19f36bv57_72qpq3mfj1x6240000gn/T/tmpuj_5ulg9/model, flavor: tensorflow). Fall back to return ['tensorflow==2.12.0', 'cloudpickle==3.0.0']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/11/03 20:04:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x7f9fca012310>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.keras.log_model(model, \"best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/03 20:10:08 INFO mlflow.tracking._tracking_service.client: 🏃 View run Early Stopping + latent factor 5 + lr 0.0014 at: http://127.0.0.1:5001/#/experiments/826553565838014976/runs/bdfe7355c87849a0951c10d2c07f5cfa.\n",
      "2024/11/03 20:10:08 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5001/#/experiments/826553565838014976.\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hay una diferencia de 1 entre n_movies, n_users y  el shape de las matrices de embeddigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1683, 3), (944, 3), 1682, 943)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_embeddings_matrix = movie_embeddings_layer.get_weights()[0]\n",
    "user_embeddings_matrix = user_embeddings_layer.get_weights()[0]\n",
    "\n",
    "movie_embeddings_matrix.shape, user_embeddings_matrix.shape, n_movies, n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/vector_db/movie_embeddings_matrix.npy', movie_embeddings_matrix)\n",
    "np.save('../data/vector_db/user_embeddings_matrix.npy', user_embeddings_matrix)\n",
    "np.save('../data/vector_db/user2Idx.npy', user2Idx)\n",
    "np.save('../data/vector_db/movie2Idx.npy', movie2Idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
