{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1997-12-04 15:55:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1998-04-04 19:22:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1997-11-07 07:18:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1997-11-27 05:02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1998-02-02 05:33:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  user_id  movie_id  rating                 Date\n",
       "0   0        1         1       3  1997-12-04 15:55:49\n",
       "1   1        2         2       3  1998-04-04 19:22:22\n",
       "2   2        3         3       1  1997-11-07 07:18:36\n",
       "3   3        4         4       2  1997-11-27 05:02:03\n",
       "4   4        5         5       1  1998-02-02 05:33:16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv('../data/scores.csv')\n",
    "df_users = pd.read_csv('../data/usuarios.csv')\n",
    "df_movies = pd.read_csv('../data/peliculas.csv')\n",
    "\n",
    "df_movies.loc[df_movies['IMDB URL'].isna(), 'IMDB URL'] = ''\n",
    "\n",
    "u_unique = ratings.user_id.unique()\n",
    "user2Idx = {o:i+1 for i,o in enumerate(u_unique)}\n",
    "\n",
    "m_unique = ratings.movie_id.unique()\n",
    "movie2Idx = {o:i+1 for i,o in enumerate(m_unique)}\n",
    "\n",
    "ratings.user_id = ratings.user_id.apply(lambda x: user2Idx[x])\n",
    "\n",
    "ratings.movie_id = ratings.movie_id.apply(lambda x: movie2Idx[x])\n",
    "\n",
    "ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ratings_train, ratings_val = train_test_split(ratings, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 1682 943 1646\n"
     ]
    }
   ],
   "source": [
    "n_users = int(ratings.user_id.nunique())\n",
    "n_movies = int(ratings.movie_id.nunique())\n",
    "n_users_train = int(ratings_train.user_id.nunique())\n",
    "n_movies_train = int(ratings_train.movie_id.nunique())\n",
    "print(n_users, n_movies, n_users_train, n_movies_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1, 3.5310125)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_rating = ratings_train['rating'].max()\n",
    "min_rating = ratings_train['rating'].min()\n",
    "av_rating = ratings_train['rating'].mean()\n",
    "max_rating, min_rating, av_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/981061820529434616', creation_time=1730667820019, experiment_id='981061820529434616', last_update_time=1730667820019, lifecycle_stage='active', name='Neural Network', tags={}>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seteo del experimento\n",
    "experiment_name = \"Neural Network\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 13:33:38.728983: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Flatten, Dropout, Concatenate, Dense, Activation, Lambda\n",
    "from keras import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.start_run(run_name=\"Latent factor 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_latent_factors_user = 5\n",
    "mlflow.log_param(\"n_latent_factors_user\", n_latent_factors_user)\n",
    "n_latent_factors_movie = 5\n",
    "mlflow.log_param(\"n_latent_factors_movie\", n_latent_factors_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Item (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " User (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " Movie-Embedding (Embedding)    (None, 1, 5)         8415        ['Item[0][0]']                   \n",
      "                                                                                                  \n",
      " User-Embedding (Embedding)     (None, 1, 5)         4720        ['User[0][0]']                   \n",
      "                                                                                                  \n",
      " FlattenMovies (Flatten)        (None, 5)            0           ['Movie-Embedding[0][0]']        \n",
      "                                                                                                  \n",
      " FlattenUsers (Flatten)         (None, 5)            0           ['User-Embedding[0][0]']         \n",
      "                                                                                                  \n",
      " Concat (Concatenate)           (None, 10)           0           ['FlattenMovies[0][0]',          \n",
      "                                                                  'FlattenUsers[0][0]']           \n",
      "                                                                                                  \n",
      " FullyConnected-1 (Dense)       (None, 50)           550         ['Concat[0][0]']                 \n",
      "                                                                                                  \n",
      " Activation (Dense)             (None, 1)            51          ['FullyConnected-1[0][0]']       \n",
      "                                                                                                  \n",
      " lambda_7 (Lambda)              (None, 1)            0           ['Activation[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,736\n",
      "Trainable params: 13,736\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "movie_embedding_regularizer = 0.001\n",
    "mlflow.log_param(\"movie_embedding_regularizer_l2\", movie_embedding_regularizer)\n",
    "\n",
    "movie_input = Input(shape=[1],name='Item')\n",
    "movie_embedding = Embedding(n_movies + 1, n_latent_factors_movie, name='Movie-Embedding', embeddings_regularizer = l2(movie_embedding_regularizer))(movie_input)\n",
    "movie_vec = Flatten(name='FlattenMovies')(movie_embedding)\n",
    "#movie_vec = Dropout(0.2)(movie_vec)\n",
    "\n",
    "user_input = Input(shape=[1],name='User')\n",
    "user_vec = Flatten(name='FlattenUsers')(Embedding(n_users + 1, \n",
    "n_latent_factors_user,name='User-Embedding')(user_input))\n",
    "#user_vec = Dropout(0.2)(user_vec)\n",
    "\n",
    "concat = Concatenate(name='Concat')([movie_vec, user_vec])\n",
    "#concat = Dropout(0.2)(concat)\n",
    "\n",
    "x = Dense(50,name='FullyConnected-1', activation='relu')(concat)\n",
    "#x = Dropout(0.5)(x)\n",
    "# x = Dense(50,name='FullyConnected-1', activation='relu')(concat)\n",
    "# x = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "## Se pueden sacar las siguientes dos lineas para no forzar a sigmoidea\n",
    "x = Dense(1, activation='sigmoid',name='Activation')(x)\n",
    "x = Lambda(lambda z: (max_rating - min_rating) * z + min_rating)(x)\n",
    "##\n",
    "\n",
    "model = Model([user_input, movie_input], x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K \n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.001\n",
    "model.compile(Adam(learning_rate=lr), 'mean_squared_error', metrics=[root_mean_squared_error])\n",
    "mlflow.log_param(\"lr\", lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True, monitor='val_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "patience = 7\n",
    "early_stopping = EarlyStopping(monitor='val_root_mean_squared_error', patience=patience, restore_best_weights=True)\n",
    "mlflow.log_param(\"early_stopping_patience\", patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.ReduceLROnPlateau at 0x18b428ee0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',      # Métrica a monitorear (puede ser 'val_loss' o 'loss')\n",
    "    factor=0.5,              # Factor de reducción del learning rate (e.g., reduce a la mitad)\n",
    "    patience=2,              # Número de épocas sin mejora antes de reducir\n",
    "    min_lr=1e-6,             # Learning rate mínimo permitido\n",
    "    verbose=1                # Mostrar logs cuando se reduzca el LR\n",
    ")\n",
    "mlflow.log_param(\"reduce_lr\", reduce_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "233/250 [==========================>...] - ETA: 0s - loss: 1.1508 - root_mean_squared_error: 1.0662\n",
      "Epoch 1: val_root_mean_squared_error improved from inf to 0.95456, saving model to weights.hdf5\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 1.1363 - root_mean_squared_error: 1.0591 - val_loss: 0.9224 - val_root_mean_squared_error: 0.9546\n",
      "Epoch 2/100\n",
      "187/250 [=====================>........] - ETA: 0s - loss: 0.8927 - root_mean_squared_error: 0.9381\n",
      "Epoch 2: val_root_mean_squared_error improved from 0.95456 to 0.94188, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8936 - root_mean_squared_error: 0.9385 - val_loss: 0.9002 - val_root_mean_squared_error: 0.9419\n",
      "Epoch 3/100\n",
      "198/250 [======================>.......] - ETA: 0s - loss: 0.8707 - root_mean_squared_error: 0.9258\n",
      "Epoch 3: val_root_mean_squared_error improved from 0.94188 to 0.93882, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8760 - root_mean_squared_error: 0.9286 - val_loss: 0.8951 - val_root_mean_squared_error: 0.9388\n",
      "Epoch 4/100\n",
      "206/250 [=======================>......] - ETA: 0s - loss: 0.8671 - root_mean_squared_error: 0.9236\n",
      "Epoch 4: val_root_mean_squared_error improved from 0.93882 to 0.93833, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8706 - root_mean_squared_error: 0.9254 - val_loss: 0.8942 - val_root_mean_squared_error: 0.9383\n",
      "Epoch 5/100\n",
      "206/250 [=======================>......] - ETA: 0s - loss: 0.8635 - root_mean_squared_error: 0.9214\n",
      "Epoch 5: val_root_mean_squared_error improved from 0.93833 to 0.93801, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8676 - root_mean_squared_error: 0.9237 - val_loss: 0.8935 - val_root_mean_squared_error: 0.9380\n",
      "Epoch 6/100\n",
      "247/250 [============================>.] - ETA: 0s - loss: 0.8656 - root_mean_squared_error: 0.9228\n",
      "Epoch 6: val_root_mean_squared_error improved from 0.93801 to 0.93726, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8657 - root_mean_squared_error: 0.9228 - val_loss: 0.8920 - val_root_mean_squared_error: 0.9373\n",
      "Epoch 7/100\n",
      "190/250 [=====================>........] - ETA: 0s - loss: 0.8572 - root_mean_squared_error: 0.9182\n",
      "Epoch 7: val_root_mean_squared_error improved from 0.93726 to 0.93579, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8631 - root_mean_squared_error: 0.9214 - val_loss: 0.8890 - val_root_mean_squared_error: 0.9358\n",
      "Epoch 8/100\n",
      "230/250 [==========================>...] - ETA: 0s - loss: 0.8577 - root_mean_squared_error: 0.9186\n",
      "Epoch 8: val_root_mean_squared_error improved from 0.93579 to 0.93394, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8580 - root_mean_squared_error: 0.9188 - val_loss: 0.8856 - val_root_mean_squared_error: 0.9339\n",
      "Epoch 9/100\n",
      "191/250 [=====================>........] - ETA: 0s - loss: 0.8490 - root_mean_squared_error: 0.9137\n",
      "Epoch 9: val_root_mean_squared_error improved from 0.93394 to 0.93137, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8497 - root_mean_squared_error: 0.9141 - val_loss: 0.8809 - val_root_mean_squared_error: 0.9314\n",
      "Epoch 10/100\n",
      "227/250 [==========================>...] - ETA: 0s - loss: 0.8408 - root_mean_squared_error: 0.9092\n",
      "Epoch 10: val_root_mean_squared_error improved from 0.93137 to 0.93011, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8421 - root_mean_squared_error: 0.9099 - val_loss: 0.8785 - val_root_mean_squared_error: 0.9301\n",
      "Epoch 11/100\n",
      "243/250 [============================>.] - ETA: 0s - loss: 0.8339 - root_mean_squared_error: 0.9053\n",
      "Epoch 11: val_root_mean_squared_error improved from 0.93011 to 0.92690, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8348 - root_mean_squared_error: 0.9058 - val_loss: 0.8729 - val_root_mean_squared_error: 0.9269\n",
      "Epoch 12/100\n",
      "244/250 [============================>.] - ETA: 0s - loss: 0.8270 - root_mean_squared_error: 0.9012\n",
      "Epoch 12: val_root_mean_squared_error did not improve from 0.92690\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8285 - root_mean_squared_error: 0.9020 - val_loss: 0.8741 - val_root_mean_squared_error: 0.9274\n",
      "Epoch 13/100\n",
      "198/250 [======================>.......] - ETA: 0s - loss: 0.8187 - root_mean_squared_error: 0.8965\n",
      "Epoch 13: val_root_mean_squared_error improved from 0.92690 to 0.92621, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8229 - root_mean_squared_error: 0.8988 - val_loss: 0.8723 - val_root_mean_squared_error: 0.9262\n",
      "Epoch 14/100\n",
      "208/250 [=======================>......] - ETA: 0s - loss: 0.8122 - root_mean_squared_error: 0.8928\n",
      "Epoch 14: val_root_mean_squared_error did not improve from 0.92621\n",
      "250/250 [==============================] - 0s 947us/step - loss: 0.8174 - root_mean_squared_error: 0.8956 - val_loss: 0.8727 - val_root_mean_squared_error: 0.9262\n",
      "Epoch 15/100\n",
      "208/250 [=======================>......] - ETA: 0s - loss: 0.8107 - root_mean_squared_error: 0.8917\n",
      "Epoch 15: val_root_mean_squared_error improved from 0.92621 to 0.92312, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8136 - root_mean_squared_error: 0.8933 - val_loss: 0.8669 - val_root_mean_squared_error: 0.9231\n",
      "Epoch 16/100\n",
      "242/250 [============================>.] - ETA: 0s - loss: 0.8078 - root_mean_squared_error: 0.8899\n",
      "Epoch 16: val_root_mean_squared_error improved from 0.92312 to 0.92278, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 0.8090 - root_mean_squared_error: 0.8906 - val_loss: 0.8665 - val_root_mean_squared_error: 0.9228\n",
      "Epoch 17/100\n",
      "231/250 [==========================>...] - ETA: 0s - loss: 0.8050 - root_mean_squared_error: 0.8882\n",
      "Epoch 17: val_root_mean_squared_error improved from 0.92278 to 0.92235, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8051 - root_mean_squared_error: 0.8883 - val_loss: 0.8660 - val_root_mean_squared_error: 0.9224\n",
      "Epoch 18/100\n",
      "219/250 [=========================>....] - ETA: 0s - loss: 0.8005 - root_mean_squared_error: 0.8856\n",
      "Epoch 18: val_root_mean_squared_error improved from 0.92235 to 0.92189, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.8014 - root_mean_squared_error: 0.8862 - val_loss: 0.8652 - val_root_mean_squared_error: 0.9219\n",
      "Epoch 19/100\n",
      "233/250 [==========================>...] - ETA: 0s - loss: 0.7976 - root_mean_squared_error: 0.8839\n",
      "Epoch 19: val_root_mean_squared_error improved from 0.92189 to 0.91989, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7982 - root_mean_squared_error: 0.8842 - val_loss: 0.8617 - val_root_mean_squared_error: 0.9199\n",
      "Epoch 20/100\n",
      "201/250 [=======================>......] - ETA: 0s - loss: 0.7882 - root_mean_squared_error: 0.8783\n",
      "Epoch 20: val_root_mean_squared_error improved from 0.91989 to 0.91794, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7933 - root_mean_squared_error: 0.8811 - val_loss: 0.8584 - val_root_mean_squared_error: 0.9179\n",
      "Epoch 21/100\n",
      "215/250 [========================>.....] - ETA: 0s - loss: 0.7864 - root_mean_squared_error: 0.8774\n",
      "Epoch 21: val_root_mean_squared_error improved from 0.91794 to 0.91693, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7882 - root_mean_squared_error: 0.8784 - val_loss: 0.8569 - val_root_mean_squared_error: 0.9169\n",
      "Epoch 22/100\n",
      "202/250 [=======================>......] - ETA: 0s - loss: 0.7829 - root_mean_squared_error: 0.8751\n",
      "Epoch 22: val_root_mean_squared_error did not improve from 0.91693\n",
      "250/250 [==============================] - 0s 949us/step - loss: 0.7821 - root_mean_squared_error: 0.8747 - val_loss: 0.8582 - val_root_mean_squared_error: 0.9174\n",
      "Epoch 23/100\n",
      "212/250 [========================>.....] - ETA: 0s - loss: 0.7725 - root_mean_squared_error: 0.8689\n",
      "Epoch 23: val_root_mean_squared_error improved from 0.91693 to 0.91492, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 973us/step - loss: 0.7768 - root_mean_squared_error: 0.8714 - val_loss: 0.8537 - val_root_mean_squared_error: 0.9149\n",
      "Epoch 24/100\n",
      "197/250 [======================>.......] - ETA: 0s - loss: 0.7643 - root_mean_squared_error: 0.8639\n",
      "Epoch 24: val_root_mean_squared_error did not improve from 0.91492\n",
      "250/250 [==============================] - 0s 967us/step - loss: 0.7713 - root_mean_squared_error: 0.8679 - val_loss: 0.8558 - val_root_mean_squared_error: 0.9158\n",
      "Epoch 25/100\n",
      "214/250 [========================>.....] - ETA: 0s - loss: 0.7674 - root_mean_squared_error: 0.8657\n",
      "Epoch 25: val_root_mean_squared_error improved from 0.91492 to 0.91449, saving model to weights.hdf5\n",
      "250/250 [==============================] - 0s 976us/step - loss: 0.7665 - root_mean_squared_error: 0.8652 - val_loss: 0.8536 - val_root_mean_squared_error: 0.9145\n",
      "Epoch 26/100\n",
      "199/250 [======================>.......] - ETA: 0s - loss: 0.7586 - root_mean_squared_error: 0.8605\n",
      "Epoch 26: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 969us/step - loss: 0.7629 - root_mean_squared_error: 0.8629 - val_loss: 0.8553 - val_root_mean_squared_error: 0.9153\n",
      "Epoch 27/100\n",
      "206/250 [=======================>......] - ETA: 0s - loss: 0.7594 - root_mean_squared_error: 0.8610\n",
      "Epoch 27: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 961us/step - loss: 0.7601 - root_mean_squared_error: 0.8613 - val_loss: 0.8581 - val_root_mean_squared_error: 0.9167\n",
      "Epoch 28/100\n",
      "195/250 [======================>.......] - ETA: 0s - loss: 0.7519 - root_mean_squared_error: 0.8565\n",
      "Epoch 28: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 992us/step - loss: 0.7562 - root_mean_squared_error: 0.8589 - val_loss: 0.8579 - val_root_mean_squared_error: 0.9165\n",
      "Epoch 29/100\n",
      "201/250 [=======================>......] - ETA: 0s - loss: 0.7510 - root_mean_squared_error: 0.8555\n",
      "Epoch 29: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 959us/step - loss: 0.7537 - root_mean_squared_error: 0.8571 - val_loss: 0.8572 - val_root_mean_squared_error: 0.9160\n",
      "Epoch 30/100\n",
      "195/250 [======================>.......] - ETA: 0s - loss: 0.7476 - root_mean_squared_error: 0.8535\n",
      "Epoch 30: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 986us/step - loss: 0.7505 - root_mean_squared_error: 0.8552 - val_loss: 0.8585 - val_root_mean_squared_error: 0.9165\n",
      "Epoch 31/100\n",
      "213/250 [========================>.....] - ETA: 0s - loss: 0.7445 - root_mean_squared_error: 0.8516\n",
      "Epoch 31: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 921us/step - loss: 0.7475 - root_mean_squared_error: 0.8534 - val_loss: 0.8584 - val_root_mean_squared_error: 0.9165\n",
      "Epoch 32/100\n",
      "212/250 [========================>.....] - ETA: 0s - loss: 0.7438 - root_mean_squared_error: 0.8510\n",
      "Epoch 32: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 936us/step - loss: 0.7447 - root_mean_squared_error: 0.8516 - val_loss: 0.8612 - val_root_mean_squared_error: 0.9178\n",
      "Epoch 33/100\n",
      "212/250 [========================>.....] - ETA: 0s - loss: 0.7399 - root_mean_squared_error: 0.8485\n",
      "Epoch 33: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 917us/step - loss: 0.7426 - root_mean_squared_error: 0.8501 - val_loss: 0.8630 - val_root_mean_squared_error: 0.9186\n",
      "Epoch 34/100\n",
      "213/250 [========================>.....] - ETA: 0s - loss: 0.7391 - root_mean_squared_error: 0.8479\n",
      "Epoch 34: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 929us/step - loss: 0.7401 - root_mean_squared_error: 0.8485 - val_loss: 0.8653 - val_root_mean_squared_error: 0.9199\n",
      "Epoch 35/100\n",
      "193/250 [======================>.......] - ETA: 0s - loss: 0.7305 - root_mean_squared_error: 0.8429\n",
      "Epoch 35: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 983us/step - loss: 0.7383 - root_mean_squared_error: 0.8474 - val_loss: 0.8681 - val_root_mean_squared_error: 0.9211\n",
      "Epoch 36/100\n",
      "212/250 [========================>.....] - ETA: 0s - loss: 0.7315 - root_mean_squared_error: 0.8435\n",
      "Epoch 36: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 920us/step - loss: 0.7363 - root_mean_squared_error: 0.8462 - val_loss: 0.8674 - val_root_mean_squared_error: 0.9208\n",
      "Epoch 37/100\n",
      "212/250 [========================>.....] - ETA: 0s - loss: 0.7297 - root_mean_squared_error: 0.8422\n",
      "Epoch 37: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 916us/step - loss: 0.7338 - root_mean_squared_error: 0.8446 - val_loss: 0.8686 - val_root_mean_squared_error: 0.9214\n",
      "Epoch 38/100\n",
      "207/250 [=======================>......] - ETA: 0s - loss: 0.7268 - root_mean_squared_error: 0.8404\n",
      "Epoch 38: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 957us/step - loss: 0.7322 - root_mean_squared_error: 0.8435 - val_loss: 0.8699 - val_root_mean_squared_error: 0.9219\n",
      "Epoch 39/100\n",
      "194/250 [======================>.......] - ETA: 0s - loss: 0.7291 - root_mean_squared_error: 0.8417\n",
      "Epoch 39: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 980us/step - loss: 0.7304 - root_mean_squared_error: 0.8424 - val_loss: 0.8708 - val_root_mean_squared_error: 0.9224\n",
      "Epoch 40/100\n",
      "207/250 [=======================>......] - ETA: 0s - loss: 0.7237 - root_mean_squared_error: 0.8384\n",
      "Epoch 40: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 950us/step - loss: 0.7281 - root_mean_squared_error: 0.8410 - val_loss: 0.8737 - val_root_mean_squared_error: 0.9240\n",
      "Epoch 41/100\n",
      "211/250 [========================>.....] - ETA: 0s - loss: 0.7236 - root_mean_squared_error: 0.8385\n",
      "Epoch 41: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 924us/step - loss: 0.7262 - root_mean_squared_error: 0.8400 - val_loss: 0.8717 - val_root_mean_squared_error: 0.9228\n",
      "Epoch 42/100\n",
      "198/250 [======================>.......] - ETA: 0s - loss: 0.7188 - root_mean_squared_error: 0.8354\n",
      "Epoch 42: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 974us/step - loss: 0.7236 - root_mean_squared_error: 0.8383 - val_loss: 0.8752 - val_root_mean_squared_error: 0.9247\n",
      "Epoch 43/100\n",
      "189/250 [=====================>........] - ETA: 0s - loss: 0.7179 - root_mean_squared_error: 0.8349\n",
      "Epoch 43: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7221 - root_mean_squared_error: 0.8375 - val_loss: 0.8752 - val_root_mean_squared_error: 0.9246\n",
      "Epoch 44/100\n",
      "215/250 [========================>.....] - ETA: 0s - loss: 0.7187 - root_mean_squared_error: 0.8352\n",
      "Epoch 44: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 909us/step - loss: 0.7198 - root_mean_squared_error: 0.8359 - val_loss: 0.8771 - val_root_mean_squared_error: 0.9256\n",
      "Epoch 45/100\n",
      "212/250 [========================>.....] - ETA: 0s - loss: 0.7158 - root_mean_squared_error: 0.8336\n",
      "Epoch 45: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 921us/step - loss: 0.7177 - root_mean_squared_error: 0.8348 - val_loss: 0.8759 - val_root_mean_squared_error: 0.9250\n",
      "Epoch 46/100\n",
      "211/250 [========================>.....] - ETA: 0s - loss: 0.7130 - root_mean_squared_error: 0.8318\n",
      "Epoch 46: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 921us/step - loss: 0.7164 - root_mean_squared_error: 0.8338 - val_loss: 0.8762 - val_root_mean_squared_error: 0.9252\n",
      "Epoch 47/100\n",
      "210/250 [========================>.....] - ETA: 0s - loss: 0.7092 - root_mean_squared_error: 0.8296\n",
      "Epoch 47: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 927us/step - loss: 0.7143 - root_mean_squared_error: 0.8325 - val_loss: 0.8778 - val_root_mean_squared_error: 0.9260\n",
      "Epoch 48/100\n",
      "215/250 [========================>.....] - ETA: 0s - loss: 0.7080 - root_mean_squared_error: 0.8287\n",
      "Epoch 48: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 917us/step - loss: 0.7125 - root_mean_squared_error: 0.8314 - val_loss: 0.8803 - val_root_mean_squared_error: 0.9271\n",
      "Epoch 49/100\n",
      "216/250 [========================>.....] - ETA: 0s - loss: 0.7066 - root_mean_squared_error: 0.8278\n",
      "Epoch 49: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.7108 - root_mean_squared_error: 0.8303 - val_loss: 0.8815 - val_root_mean_squared_error: 0.9278\n",
      "Epoch 50/100\n",
      "214/250 [========================>.....] - ETA: 0s - loss: 0.7046 - root_mean_squared_error: 0.8265\n",
      "Epoch 50: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 911us/step - loss: 0.7090 - root_mean_squared_error: 0.8291 - val_loss: 0.8843 - val_root_mean_squared_error: 0.9293\n",
      "Epoch 51/100\n",
      "213/250 [========================>.....] - ETA: 0s - loss: 0.7023 - root_mean_squared_error: 0.8251\n",
      "Epoch 51: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 913us/step - loss: 0.7074 - root_mean_squared_error: 0.8282 - val_loss: 0.8840 - val_root_mean_squared_error: 0.9291\n",
      "Epoch 52/100\n",
      "214/250 [========================>.....] - ETA: 0s - loss: 0.6995 - root_mean_squared_error: 0.8235\n",
      "Epoch 52: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 916us/step - loss: 0.7062 - root_mean_squared_error: 0.8275 - val_loss: 0.8876 - val_root_mean_squared_error: 0.9308\n",
      "Epoch 53/100\n",
      "214/250 [========================>.....] - ETA: 0s - loss: 0.7011 - root_mean_squared_error: 0.8242\n",
      "Epoch 53: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 913us/step - loss: 0.7046 - root_mean_squared_error: 0.8263 - val_loss: 0.8884 - val_root_mean_squared_error: 0.9313\n",
      "Epoch 54/100\n",
      "211/250 [========================>.....] - ETA: 0s - loss: 0.6987 - root_mean_squared_error: 0.8227\n",
      "Epoch 54: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.7029 - root_mean_squared_error: 0.8252 - val_loss: 0.8863 - val_root_mean_squared_error: 0.9302\n",
      "Epoch 55/100\n",
      "212/250 [========================>.....] - ETA: 0s - loss: 0.6994 - root_mean_squared_error: 0.8230\n",
      "Epoch 55: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 923us/step - loss: 0.7020 - root_mean_squared_error: 0.8246 - val_loss: 0.8901 - val_root_mean_squared_error: 0.9321\n",
      "Epoch 56/100\n",
      "211/250 [========================>.....] - ETA: 0s - loss: 0.6947 - root_mean_squared_error: 0.8202\n",
      "Epoch 56: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 931us/step - loss: 0.7002 - root_mean_squared_error: 0.8234 - val_loss: 0.8873 - val_root_mean_squared_error: 0.9307\n",
      "Epoch 57/100\n",
      "215/250 [========================>.....] - ETA: 0s - loss: 0.6986 - root_mean_squared_error: 0.8225\n",
      "Epoch 57: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6994 - root_mean_squared_error: 0.8230 - val_loss: 0.8892 - val_root_mean_squared_error: 0.9316\n",
      "Epoch 58/100\n",
      "215/250 [========================>.....] - ETA: 0s - loss: 0.6946 - root_mean_squared_error: 0.8201\n",
      "Epoch 58: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 914us/step - loss: 0.6973 - root_mean_squared_error: 0.8218 - val_loss: 0.8915 - val_root_mean_squared_error: 0.9328\n",
      "Epoch 59/100\n",
      "203/250 [=======================>......] - ETA: 0s - loss: 0.6906 - root_mean_squared_error: 0.8176\n",
      "Epoch 59: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 942us/step - loss: 0.6962 - root_mean_squared_error: 0.8210 - val_loss: 0.8956 - val_root_mean_squared_error: 0.9350\n",
      "Epoch 60/100\n",
      "215/250 [========================>.....] - ETA: 0s - loss: 0.6931 - root_mean_squared_error: 0.8191\n",
      "Epoch 60: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 915us/step - loss: 0.6953 - root_mean_squared_error: 0.8205 - val_loss: 0.8955 - val_root_mean_squared_error: 0.9349\n",
      "Epoch 61/100\n",
      "230/250 [==========================>...] - ETA: 0s - loss: 0.6955 - root_mean_squared_error: 0.8204\n",
      "Epoch 61: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6939 - root_mean_squared_error: 0.8195 - val_loss: 0.8923 - val_root_mean_squared_error: 0.9331\n",
      "Epoch 62/100\n",
      "211/250 [========================>.....] - ETA: 0s - loss: 0.6891 - root_mean_squared_error: 0.8166\n",
      "Epoch 62: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 923us/step - loss: 0.6928 - root_mean_squared_error: 0.8189 - val_loss: 0.8951 - val_root_mean_squared_error: 0.9347\n",
      "Epoch 63/100\n",
      "211/250 [========================>.....] - ETA: 0s - loss: 0.6870 - root_mean_squared_error: 0.8152\n",
      "Epoch 63: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 936us/step - loss: 0.6915 - root_mean_squared_error: 0.8180 - val_loss: 0.8969 - val_root_mean_squared_error: 0.9355\n",
      "Epoch 64/100\n",
      "209/250 [========================>.....] - ETA: 0s - loss: 0.6864 - root_mean_squared_error: 0.8148\n",
      "Epoch 64: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 932us/step - loss: 0.6907 - root_mean_squared_error: 0.8174 - val_loss: 0.8955 - val_root_mean_squared_error: 0.9348\n",
      "Epoch 65/100\n",
      "209/250 [========================>.....] - ETA: 0s - loss: 0.6860 - root_mean_squared_error: 0.8146\n",
      "Epoch 65: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 932us/step - loss: 0.6894 - root_mean_squared_error: 0.8167 - val_loss: 0.8971 - val_root_mean_squared_error: 0.9356\n",
      "Epoch 66/100\n",
      "208/250 [=======================>......] - ETA: 0s - loss: 0.6854 - root_mean_squared_error: 0.8142\n",
      "Epoch 66: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6889 - root_mean_squared_error: 0.8163 - val_loss: 0.8972 - val_root_mean_squared_error: 0.9357\n",
      "Epoch 67/100\n",
      "212/250 [========================>.....] - ETA: 0s - loss: 0.6844 - root_mean_squared_error: 0.8135\n",
      "Epoch 67: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 925us/step - loss: 0.6876 - root_mean_squared_error: 0.8155 - val_loss: 0.8977 - val_root_mean_squared_error: 0.9360\n",
      "Epoch 68/100\n",
      "208/250 [=======================>......] - ETA: 0s - loss: 0.6766 - root_mean_squared_error: 0.8087\n",
      "Epoch 68: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 940us/step - loss: 0.6859 - root_mean_squared_error: 0.8143 - val_loss: 0.9022 - val_root_mean_squared_error: 0.9383\n",
      "Epoch 69/100\n",
      "202/250 [=======================>......] - ETA: 0s - loss: 0.6806 - root_mean_squared_error: 0.8112\n",
      "Epoch 69: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 974us/step - loss: 0.6847 - root_mean_squared_error: 0.8138 - val_loss: 0.9011 - val_root_mean_squared_error: 0.9378\n",
      "Epoch 70/100\n",
      "192/250 [======================>.......] - ETA: 0s - loss: 0.6799 - root_mean_squared_error: 0.8107\n",
      "Epoch 70: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6842 - root_mean_squared_error: 0.8134 - val_loss: 0.9046 - val_root_mean_squared_error: 0.9396\n",
      "Epoch 71/100\n",
      "224/250 [=========================>....] - ETA: 0s - loss: 0.6791 - root_mean_squared_error: 0.8101\n",
      "Epoch 71: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6825 - root_mean_squared_error: 0.8122 - val_loss: 0.9064 - val_root_mean_squared_error: 0.9405\n",
      "Epoch 72/100\n",
      "209/250 [========================>.....] - ETA: 0s - loss: 0.6777 - root_mean_squared_error: 0.8093\n",
      "Epoch 72: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 934us/step - loss: 0.6818 - root_mean_squared_error: 0.8117 - val_loss: 0.9046 - val_root_mean_squared_error: 0.9395\n",
      "Epoch 73/100\n",
      "201/250 [=======================>......] - ETA: 0s - loss: 0.6744 - root_mean_squared_error: 0.8072\n",
      "Epoch 73: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 965us/step - loss: 0.6812 - root_mean_squared_error: 0.8113 - val_loss: 0.9063 - val_root_mean_squared_error: 0.9405\n",
      "Epoch 74/100\n",
      "212/250 [========================>.....] - ETA: 0s - loss: 0.6770 - root_mean_squared_error: 0.8088\n",
      "Epoch 74: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 927us/step - loss: 0.6795 - root_mean_squared_error: 0.8103 - val_loss: 0.9095 - val_root_mean_squared_error: 0.9422\n",
      "Epoch 75/100\n",
      "207/250 [=======================>......] - ETA: 0s - loss: 0.6760 - root_mean_squared_error: 0.8082\n",
      "Epoch 75: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 940us/step - loss: 0.6791 - root_mean_squared_error: 0.8102 - val_loss: 0.9062 - val_root_mean_squared_error: 0.9405\n",
      "Epoch 76/100\n",
      "217/250 [=========================>....] - ETA: 0s - loss: 0.6767 - root_mean_squared_error: 0.8086\n",
      "Epoch 76: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6784 - root_mean_squared_error: 0.8097 - val_loss: 0.9126 - val_root_mean_squared_error: 0.9438\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6779 - root_mean_squared_error: 0.8093\n",
      "Epoch 77: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6779 - root_mean_squared_error: 0.8093 - val_loss: 0.9107 - val_root_mean_squared_error: 0.9428\n",
      "Epoch 78/100\n",
      "201/250 [=======================>......] - ETA: 0s - loss: 0.6746 - root_mean_squared_error: 0.8073\n",
      "Epoch 78: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 981us/step - loss: 0.6771 - root_mean_squared_error: 0.8089 - val_loss: 0.9136 - val_root_mean_squared_error: 0.9442\n",
      "Epoch 79/100\n",
      "202/250 [=======================>......] - ETA: 0s - loss: 0.6718 - root_mean_squared_error: 0.8054\n",
      "Epoch 79: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 952us/step - loss: 0.6754 - root_mean_squared_error: 0.8077 - val_loss: 0.9118 - val_root_mean_squared_error: 0.9433\n",
      "Epoch 80/100\n",
      "200/250 [=======================>......] - ETA: 0s - loss: 0.6720 - root_mean_squared_error: 0.8055\n",
      "Epoch 80: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 990us/step - loss: 0.6754 - root_mean_squared_error: 0.8077 - val_loss: 0.9116 - val_root_mean_squared_error: 0.9433\n",
      "Epoch 81/100\n",
      "191/250 [=====================>........] - ETA: 0s - loss: 0.6658 - root_mean_squared_error: 0.8018\n",
      "Epoch 81: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6743 - root_mean_squared_error: 0.8071 - val_loss: 0.9149 - val_root_mean_squared_error: 0.9449\n",
      "Epoch 82/100\n",
      "189/250 [=====================>........] - ETA: 0s - loss: 0.6659 - root_mean_squared_error: 0.8016\n",
      "Epoch 82: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6734 - root_mean_squared_error: 0.8063 - val_loss: 0.9152 - val_root_mean_squared_error: 0.9452\n",
      "Epoch 83/100\n",
      "196/250 [======================>.......] - ETA: 0s - loss: 0.6677 - root_mean_squared_error: 0.8029\n",
      "Epoch 83: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 989us/step - loss: 0.6733 - root_mean_squared_error: 0.8064 - val_loss: 0.9123 - val_root_mean_squared_error: 0.9436\n",
      "Epoch 84/100\n",
      "192/250 [======================>.......] - ETA: 0s - loss: 0.6647 - root_mean_squared_error: 0.8010\n",
      "Epoch 84: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 999us/step - loss: 0.6721 - root_mean_squared_error: 0.8056 - val_loss: 0.9192 - val_root_mean_squared_error: 0.9472\n",
      "Epoch 85/100\n",
      "185/250 [=====================>........] - ETA: 0s - loss: 0.6691 - root_mean_squared_error: 0.8037\n",
      "Epoch 85: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6717 - root_mean_squared_error: 0.8053 - val_loss: 0.9174 - val_root_mean_squared_error: 0.9463\n",
      "Epoch 86/100\n",
      "211/250 [========================>.....] - ETA: 0s - loss: 0.6669 - root_mean_squared_error: 0.8023\n",
      "Epoch 86: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6709 - root_mean_squared_error: 0.8047 - val_loss: 0.9179 - val_root_mean_squared_error: 0.9464\n",
      "Epoch 87/100\n",
      "207/250 [=======================>......] - ETA: 0s - loss: 0.6667 - root_mean_squared_error: 0.8021\n",
      "Epoch 87: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 937us/step - loss: 0.6703 - root_mean_squared_error: 0.8043 - val_loss: 0.9179 - val_root_mean_squared_error: 0.9464\n",
      "Epoch 88/100\n",
      "210/250 [========================>.....] - ETA: 0s - loss: 0.6649 - root_mean_squared_error: 0.8011\n",
      "Epoch 88: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 927us/step - loss: 0.6693 - root_mean_squared_error: 0.8038 - val_loss: 0.9171 - val_root_mean_squared_error: 0.9460\n",
      "Epoch 89/100\n",
      "214/250 [========================>.....] - ETA: 0s - loss: 0.6643 - root_mean_squared_error: 0.8006\n",
      "Epoch 89: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 939us/step - loss: 0.6687 - root_mean_squared_error: 0.8034 - val_loss: 0.9196 - val_root_mean_squared_error: 0.9472\n",
      "Epoch 90/100\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.6674 - root_mean_squared_error: 0.8027\n",
      "Epoch 90: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6677 - root_mean_squared_error: 0.8028 - val_loss: 0.9260 - val_root_mean_squared_error: 0.9507\n",
      "Epoch 91/100\n",
      "211/250 [========================>.....] - ETA: 0s - loss: 0.6623 - root_mean_squared_error: 0.7994\n",
      "Epoch 91: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 940us/step - loss: 0.6676 - root_mean_squared_error: 0.8027 - val_loss: 0.9203 - val_root_mean_squared_error: 0.9477\n",
      "Epoch 92/100\n",
      "211/250 [========================>.....] - ETA: 0s - loss: 0.6596 - root_mean_squared_error: 0.7978\n",
      "Epoch 92: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 925us/step - loss: 0.6670 - root_mean_squared_error: 0.8024 - val_loss: 0.9196 - val_root_mean_squared_error: 0.9473\n",
      "Epoch 93/100\n",
      "214/250 [========================>.....] - ETA: 0s - loss: 0.6604 - root_mean_squared_error: 0.7983\n",
      "Epoch 93: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 921us/step - loss: 0.6665 - root_mean_squared_error: 0.8021 - val_loss: 0.9227 - val_root_mean_squared_error: 0.9489\n",
      "Epoch 94/100\n",
      "210/250 [========================>.....] - ETA: 0s - loss: 0.6602 - root_mean_squared_error: 0.7979\n",
      "Epoch 94: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 930us/step - loss: 0.6655 - root_mean_squared_error: 0.8013 - val_loss: 0.9225 - val_root_mean_squared_error: 0.9489\n",
      "Epoch 95/100\n",
      "212/250 [========================>.....] - ETA: 0s - loss: 0.6646 - root_mean_squared_error: 0.8008\n",
      "Epoch 95: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 922us/step - loss: 0.6657 - root_mean_squared_error: 0.8015 - val_loss: 0.9238 - val_root_mean_squared_error: 0.9494\n",
      "Epoch 96/100\n",
      "207/250 [=======================>......] - ETA: 0s - loss: 0.6643 - root_mean_squared_error: 0.8007\n",
      "Epoch 96: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 937us/step - loss: 0.6656 - root_mean_squared_error: 0.8015 - val_loss: 0.9225 - val_root_mean_squared_error: 0.9488\n",
      "Epoch 97/100\n",
      "214/250 [========================>.....] - ETA: 0s - loss: 0.6591 - root_mean_squared_error: 0.7974\n",
      "Epoch 97: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 919us/step - loss: 0.6645 - root_mean_squared_error: 0.8007 - val_loss: 0.9229 - val_root_mean_squared_error: 0.9490\n",
      "Epoch 98/100\n",
      "213/250 [========================>.....] - ETA: 0s - loss: 0.6625 - root_mean_squared_error: 0.7995\n",
      "Epoch 98: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 927us/step - loss: 0.6641 - root_mean_squared_error: 0.8006 - val_loss: 0.9221 - val_root_mean_squared_error: 0.9486\n",
      "Epoch 99/100\n",
      "241/250 [===========================>..] - ETA: 0s - loss: 0.6617 - root_mean_squared_error: 0.7990\n",
      "Epoch 99: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.6631 - root_mean_squared_error: 0.7998 - val_loss: 0.9214 - val_root_mean_squared_error: 0.9483\n",
      "Epoch 100/100\n",
      "213/250 [========================>.....] - ETA: 0s - loss: 0.6598 - root_mean_squared_error: 0.7978\n",
      "Epoch 100: val_root_mean_squared_error did not improve from 0.91449\n",
      "250/250 [==============================] - 0s 923us/step - loss: 0.6628 - root_mean_squared_error: 0.7997 - val_loss: 0.9253 - val_root_mean_squared_error: 0.9502\n"
     ]
    }
   ],
   "source": [
    "batch_size = 320\n",
    "epochs = 100\n",
    "mlflow.log_param(\"batch_size\", batch_size)\n",
    "mlflow.log_param(\"epochs\", epochs)\n",
    "\n",
    "history = model.fit([ratings_train.user_id, ratings_train.movie_id], \n",
    "                    ratings_train.rating, \n",
    "                    validation_data=([ratings_val.user_id, ratings_val.movie_id], ratings_val.rating), \n",
    "                    batch_size = batch_size,\n",
    "                    callbacks = [checkpointer],\n",
    "                    epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in history.history.items():\n",
    "  mlflow.log_metric(key, value[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 648us/step - loss: 0.8872 - root_mean_squared_error: 0.9263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8871934413909912, 0.9262680411338806]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([ratings_val.user_id, ratings_val.movie_id], ratings_val.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 602us/step - loss: 0.8536 - root_mean_squared_error: 0.9060\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('weights.hdf5')\n",
    "mse, rmse = model.evaluate([ratings_val.user_id, ratings_val.movie_id], ratings_val.rating)\n",
    "mlflow.log_metric(\"val_mse\", mse) \n",
    "mlflow.log_metric(\"val_rmse\", rmse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Movie-Embedding', 'User-Embedding')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_embeddings_layer = model.layers[2]\n",
    "user_embeddings_layer = model.layers[3]\n",
    "\n",
    "movie_embeddings_layer.name, user_embeddings_layer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/28 14:11:13 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) User, Item with unsupported characters which will be renamed to user, item in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/7f/19f36bv57_72qpq3mfj1x6240000gn/T/tmpolxub5_u/model/data/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/7f/19f36bv57_72qpq3mfj1x6240000gn/T/tmpolxub5_u/model/data/model/assets\n",
      "2024/11/28 14:11:19 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /var/folders/7f/19f36bv57_72qpq3mfj1x6240000gn/T/tmpolxub5_u/model, flavor: tensorflow). Fall back to return ['tensorflow==2.11.0']. Set logging level to DEBUG to see the full traceback. \n",
      "2024/11/28 14:11:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x18b6e1a00>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.keras.log_model(model, \"best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/28 14:11:22 INFO mlflow.tracking._tracking_service.client: 🏃 View run Latent factor 5 at: http://127.0.0.1:5001/#/experiments/981061820529434616/runs/bb6fb7b6bf92488bbc6717fcb20c1368.\n",
      "2024/11/28 14:11:22 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5001/#/experiments/981061820529434616.\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hay una diferencia de 1 entre n_movies, n_users y  el shape de las matrices de embeddigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1683, 5), (944, 5), 1682, 943)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_embeddings_matrix = movie_embeddings_layer.get_weights()[0]\n",
    "user_embeddings_matrix = user_embeddings_layer.get_weights()[0]\n",
    "\n",
    "movie_embeddings_matrix.shape, user_embeddings_matrix.shape, n_movies, n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('../data/vector_db/movie_embeddings_matrix.npy', movie_embeddings_matrix)\n",
    "np.save('../data/vector_db/user_embeddings_matrix.npy', user_embeddings_matrix)\n",
    "np.save('../data/vector_db/user2Idx.npy', user2Idx)\n",
    "np.save('../data/vector_db/movie2Idx.npy', movie2Idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
